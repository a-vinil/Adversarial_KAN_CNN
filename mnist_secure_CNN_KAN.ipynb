{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb2ab4-c4bd-4a61-9a85-5347064c0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# KANLinear definition Soure: https://github.com/Blealtan/efficient-kan/blob/f39e5146af34299ad3a581d2106eb667ba0fa6fa/src/efficient_kan/kan.py#L6\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=100,\n",
    "        spline_order=5,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "# CNN model for CIFAR-10 with KANLinear\n",
    "class CNNKAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNKAN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.kan1 = KANLinear(64 * 7 * 7, 256)  \n",
    "        self.kan2 = KANLinear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.selu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.kan1(x)\n",
    "        x = self.kan2(x)\n",
    "        return x\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  \n",
    "        self.fc2 = nn.Linear(256, 10)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.selu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flattening the layer for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.selu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "def print_parameter_details(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            params = parameter.numel()  # Number of elements in the tensor\n",
    "            total_params += params\n",
    "            print(f\"{name}: {params}\")\n",
    "    print(f\"Total trainable parameters: {total_params}\") \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CNN().to(device) \n",
    "\n",
    "# Uncommnet this line for CNN KAN. \n",
    "model = CNNKAN().to(device) \n",
    "print(model) \n",
    "print_parameter_details(model)\n",
    "summary(model,  input_size=(1, 28, 28))\n",
    "\n",
    "# Note the this is just a rough demo for Visualization. Need modifcation. \n",
    "def visualize_kan_parameters(kan_layer, layer_name):\n",
    "    base_weights = kan_layer.base_weight.data.cpu().numpy()\n",
    "    plt.hist(base_weights.ravel(), bins=50)\n",
    "    plt.title(f\"Distribution of Base Weights - {layer_name}\")\n",
    "    plt.xlabel(\"Weight Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    if hasattr(kan_layer, 'spline_weight'):\n",
    "        spline_weights = kan_layer.spline_weight.data.cpu().numpy()\n",
    "        plt.hist(spline_weights.ravel(), bins=50)\n",
    "        plt.title(f\"Distribution of Spline Weights - {layer_name}\")\n",
    "        plt.xlabel(\"Weight Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()} {'requires_grad' if param.requires_grad else 'frozen'}\")\n",
    "\n",
    "# TODO: Need to explore various Optimizer and optimize the Learning Rate.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((28, 28)),  # Resize if needed\n",
    "    transforms.ToTensor(),  # Convert to Tensor\n",
    "])\n",
    "\n",
    "#########################\n",
    "class AdversarialDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Extract label from filename (assuming filename format is 'index_labelX.png')\n",
    "        label = int(image_path.split('_label')[1].split('.png')[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class AdversarialDatasetVisualize(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted(os.listdir(image_dir))  # Sort to align with train images for comparison\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "#########################\n",
    "\n",
    "\n",
    "train_adv_dir = './mnist_fgsm2/train_adv'\n",
    "test_adv_dir = './mnist_fgsm2/test_adv'\n",
    "\n",
    "# Load normal MNIST dataset\n",
    "normal_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "normal_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Load adversarial MNIST dataset\n",
    "adversarial_train_dataset = AdversarialDataset(train_adv_dir, transform=transform)\n",
    "adversarial_test_dataset = AdversarialDataset(test_adv_dir, transform=transform)\n",
    "\n",
    "# Determine the number of samples to take from each dataset (half of the size of the smaller dataset)\n",
    "num_samples = min(len(normal_train_dataset), len(adversarial_train_dataset)) // 2\n",
    "\n",
    "# Slice datasets to take half of the images from each\n",
    "normal_train_subset = torch.utils.data.Subset(normal_train_dataset, range(num_samples))\n",
    "adversarial_train_subset = torch.utils.data.Subset(adversarial_train_dataset, range(num_samples))\n",
    "\n",
    "# Combine the normal and adversarial datasets\n",
    "combined_train_dataset = ConcatDataset([normal_train_subset, adversarial_train_subset])\n",
    "\n",
    "# Similarly, slice for the test datasets\n",
    "num_test_samples = min(len(normal_test_dataset), len(adversarial_test_dataset)) // 2\n",
    "normal_test_subset = torch.utils.data.Subset(normal_test_dataset, range(num_test_samples))\n",
    "adversarial_test_subset = torch.utils.data.Subset(adversarial_test_dataset, range(num_test_samples))\n",
    "\n",
    "# Combine the test datasets\n",
    "combined_test_dataset = ConcatDataset([normal_test_subset, adversarial_test_subset])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(combined_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f'Combined Train dataset size: {len(combined_train_dataset)}')\n",
    "print(f'Combined Test dataset size: {len(combined_test_dataset)}')\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())  # Collect predictions\n",
    "            all_targets.extend(target.cpu().numpy())  # Collect true targets\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate F1 score (average='weighted' handles multi-class)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')\n",
    "    print(f'F1 Score: {f1:.4f}\\n')\n",
    "\n",
    "test_adv_dataset = AdversarialDatasetVisualize(test_adv_dir, transform=transform)\n",
    "\n",
    "def visualize_images(train_dataset, test_adv_dataset, num_images=5):\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(8, 2 * num_images))\n",
    "    fig.suptitle(\"Normal Train Images vs. Adversarial Test Images\", fontsize=16)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Normal image\n",
    "        normal_img, _ = train_dataset[i]\n",
    "        normal_img = normal_img.permute(1, 2, 0)  # Move channels for visualization\n",
    "        normal_img = normal_img * torch.tensor((0.2023, 0.1994, 0.2010)) + torch.tensor((0.4914, 0.4822, 0.4465))  # Unnormalize\n",
    "\n",
    "        # Adversarial image\n",
    "        adv_img = test_adv_dataset[i]\n",
    "        adv_img = adv_img.permute(1, 2, 0)\n",
    "        adv_img = adv_img * torch.tensor((0.2023, 0.1994, 0.2010)) + torch.tensor((0.4914, 0.4822, 0.4465))  # Unnormalize\n",
    "\n",
    "        # Plot the images\n",
    "        axes[i, 0].imshow(normal_img)\n",
    "        axes[i, 0].set_title(\"Normal Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(adv_img)\n",
    "        axes[i, 1].set_title(\"Adversarial Image\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Display images\n",
    "visualize_images(normal_train_dataset, test_adv_dataset)\n",
    "    \n",
    "for epoch in range(5):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    evaluate(model, device, test_loader)\n",
    "torch.save(model.state_dict(), 'model_weights_KAN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
